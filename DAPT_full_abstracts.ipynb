{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DAPT full abstracts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schwarzmarcel/MasterThesis_MSchwarz/blob/main/DAPT_full_abstracts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn7AWxNDLhkN"
      },
      "source": [
        "# Install and import necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skxpVe7yWYBm"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2bmwFAMW0mI"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils import data \n",
        "from sklearn.model_selection import train_test_split\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_FOR_MASKED_LM_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    HfArgumentParser,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        "    RobertaModel, \n",
        "    RobertaTokenizerFast, \n",
        "    RobertaForMaskedLM\n",
        ")\n",
        "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "from matplotlib import rc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqzrkKPGYME2"
      },
      "source": [
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqijFrllLm_g"
      },
      "source": [
        "# Check for GPU\n",
        "\n",
        "Tesla P100 is recommended since this procedure takes a few hours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lepX8X-QW6hW"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv6P74VaXda4"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQorbMdYXfvc"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N01bgNP3Mqhl"
      },
      "source": [
        "# Prepare Data\n",
        "\n",
        "\n",
        "*   load unlabeled dataset with >31,500 abstracts\n",
        "*   reduce size of dataset to 80%; this is done because the training takes too long with the full set on RoBERTa; not necessary with DistilBERT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGZxcagpbJCv"
      },
      "source": [
        "df = pd.read_json(\"pubmed_papers.json\")\n",
        "df = df.drop(columns=[\"Doi\", \"PMID\", \"Authors\", \"Title\", \"Abstract\", \"Extractive\", \"Abstractive\", \"Methods\"])\n",
        "df = df.rename(columns={\"Sentences\": \"text\"})\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AuwMT1SlWL7"
      },
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddLVRlAVHPoE"
      },
      "source": [
        "df_train = df_train.reset_index(drop=True)\n",
        "df_test = df_test.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo6B2Uxr-fCu"
      },
      "source": [
        "# reduce dataset\n",
        "df_train = df_train[:20176]\n",
        "df_test = df_test[:5044]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIwkf7U-neqq"
      },
      "source": [
        "# transform abstract from sentence list to single string\n",
        "df_train[\"text\"] = df_train[\"text\"].apply(lambda x: (\" \").join(x))\n",
        "df_test[\"text\"] = df_test[\"text\"].apply(lambda x: (\" \").join(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Xeei9VaOSSU"
      },
      "source": [
        "dataset_train = Dataset.from_pandas(df_train)\n",
        "dataset_test = Dataset.from_pandas(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh-kAv1QNcD1"
      },
      "source": [
        "# Define model and tokenization function\n",
        "\n",
        "This implementation uses a lot of the prepared functions from the hugginface library which makes the training process easier. It works pretty much the same way as the previous DAPT notebook which I used for BERT. \n",
        "\n",
        "The tokenization function uses the same special token positioning as for the fine-tuning task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO_pZoLgcywT"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = \"roberta-base\" # \"distilbert-base-uncased\"\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(PRE_TRAINED_MODEL_NAME) # DistilBertTokenizerFast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL9uS5TFY4LA"
      },
      "source": [
        "model = AutoModelForMaskedLM.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUWWX9FLZfIc"
      },
      "source": [
        "MAX_LEN = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOX5oklJVZwO"
      },
      "source": [
        "def tokenize_function(examples):\n",
        "  input_ids = []\n",
        "  attention_mask = []\n",
        "  special_tokens_mask = []\n",
        "  for sentence in examples[\"text\"]:\n",
        "    encoded = tokenizer(\n",
        "        sentence,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_special_tokens_mask=True\n",
        "    )\n",
        "    input_ids.extend(encoded[\"input_ids\"])\n",
        "    attention_mask.extend(encoded[\"attention_mask\"])\n",
        "    special_tokens_mask.extend(encoded[\"special_tokens_mask\"])\n",
        "  if len(input_ids) > MAX_LEN:\n",
        "    input_ids = input_ids[:MAX_LEN]\n",
        "    attention_mask = attention_mask[:MAX_LEN]\n",
        "    special_tokens_mask = special_tokens_mask[:MAX_LEN]\n",
        "    input_ids[MAX_LEN-1] = tokenizer.sep_token_id\n",
        "    special_tokens_mask[MAX_LEN-1] = 1\n",
        "  elif len(input_ids) < MAX_LEN:\n",
        "    padding_ids = [1] * (MAX_LEN - len(input_ids))\n",
        "    padding_attn = [0] * (MAX_LEN - len(input_ids))\n",
        "    input_ids.extend(padding_ids)\n",
        "    attention_mask.extend(padding_attn)\n",
        "    special_tokens_mask.extend(padding_ids)\n",
        "  \n",
        "  return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"attention_mask\": attention_mask,\n",
        "      \"special_tokens_mask\": special_tokens_mask \n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHjq5j7vQpkx"
      },
      "source": [
        "tokenized_train = dataset_train.map(\n",
        "    tokenize_function,\n",
        "    remove_columns=[\"text\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2xV2rCNRT_x"
      },
      "source": [
        "tokenized_test = dataset_test.map(\n",
        "    tokenize_function,\n",
        "    remove_columns=[\"text\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grjXmoAebh1Q"
      },
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIIpAl0lPTko"
      },
      "source": [
        "# Training and validation\n",
        "\n",
        "Here the actual DAPT happens. The hyperparameters for training must be defined and then the Trainer from huggingface does the work for us. During the training process some metrics are returned after every epoch. On a Tesla P100 this should take ~5 hours for RoBERTa and ~3.5h with the full dataset for DistilBERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wp7OC_XhGQS"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = \"\",\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    per_device_train_batch_size = 10,\n",
        "    per_device_eval_batch_size = 10,\n",
        "    learning_rate = 6e-4, #1e-4, 3e-5\n",
        "    weight_decay = 0.01,\n",
        "    adam_beta2 = 0.98,\n",
        "    adam_epsilon = 1e-6,\n",
        "    max_grad_norm = 0.0,\n",
        "    num_train_epochs = 10.0,\n",
        "    warmup_ratio = 0.06,\n",
        "    save_steps=5000,\n",
        "    seed = RANDOM_SEED,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6TCAqpvdDSm"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTeH_eJ0bKeT"
      },
      "source": [
        "last_checkpoint = None \n",
        "model_path = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31maBjvkayoz"
      },
      "source": [
        "%%time\n",
        "\n",
        "if last_checkpoint is not None:\n",
        "  checkpoint = last_checkpoint\n",
        "elif model_path is not None:\n",
        "  checkpoint = model_path\n",
        "else:\n",
        "  checkpoint = None\n",
        "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "metrics = train_result.metrics\n",
        "print(metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw5fs49GsBiK"
      },
      "source": [
        "model.roberta.save_pretrained(\"roberta_dapt\") "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}