{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DAPT BERT sentence pairs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1lXst4YqQoHy323pWoWFDKFXOk0Ty4Qbt",
      "authorship_tag": "ABX9TyMzvD11rjDRE7PWSqb56Egp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schwarzmarcel/MasterThesis_MSchwarz/blob/main/DAPT_BERT_sentence_pairs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETiNaoOdEIMQ"
      },
      "source": [
        "# Install and import necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKabmf1l09d"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpRJ_Dn1nHFB"
      },
      "source": [
        "from transformers import BertModel, BertTokenizerFast, BertForMaskedLM, BertForNextSentencePrediction\n",
        "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
        "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
        "import torch \n",
        "import copy\n",
        "import random\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "from torch.utils import data \n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw-1SzliEO_w"
      },
      "source": [
        "# Check for GPU\n",
        "\n",
        "Tesla P100 is recommended since this procedure takes a few hours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6YfFlGRnh7P"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD94uHOTnd5v"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVXpPxI5neou"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G0zmpP4Ehho"
      },
      "source": [
        "# Prepare Data\n",
        "\n",
        " \n",
        "\n",
        "*   open the dataset containing setnence pairs (50% of the time the sentence B is the subsequent sentence to sentence A; the other 50% sentence B is a random sentence from the corpus)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8s8gF0unPCK"
      },
      "source": [
        "df = pd.read_json(\"sentence_pairs.json\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMXSoRAf39N6"
      },
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.7, random_state=RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCvXlfg5G0LS"
      },
      "source": [
        "# Select model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJRfxe1RnnnS"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = \"bert-base-cased\"\n",
        "tokenizer = BertTokenizerFast.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ulkdkq7oGv0"
      },
      "source": [
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arEL6HEMHP7a"
      },
      "source": [
        "# Tokenizer and DataLoaders\n",
        "\n",
        "The tokenizer creates the input for the pre-trained LMs and does the padding/truncation. This is simpler for single sentence input. The necessary outputs are:\n",
        "\n",
        "*   Standard inputs: input_ids, attention_mask, segment_ids(here called token_type_ids)\n",
        "*   label for NSP: is_next\n",
        "*   labels for MLM: mask_labels\n",
        "\n",
        "The tokenization function truncates sentence pairs that are too long. If one sentence is much longer than the other one (difference >25 tokens) then only the longer sentence is truncated. Else both sentences are truncated equally.\n",
        "\n",
        "Then, tokens are randomly masked for each sentence with the BERT masking strategy (random_word function). \n",
        "\n",
        "The dataloaders create batches from the dataset which can be used by the LM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx80P2hWtrTH"
      },
      "source": [
        "def random_word(tokens, tokenizer):\n",
        "\n",
        "    output_label = [-100]*len(tokens)\n",
        "    i = 0\n",
        "    while i < len(tokens)-1:\n",
        "      prob = random.random()\n",
        "\n",
        "      if prob < 0.15:\n",
        "          prob /= 0.15\n",
        "\n",
        "          output_label[i] = tokens[i]\n",
        "\n",
        "          if prob < 0.8:\n",
        "              tokens[i] = tokenizer.mask_token_id\n",
        "\n",
        "          elif prob < 0.9:\n",
        "              tokens[i] = random.choice(list(tokenizer.vocab.items()))[1]\n",
        "\n",
        "      i+=1\n",
        "\n",
        "    return tokens, output_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vExkc_myPomF"
      },
      "source": [
        "def tokenize_pairs(sentence_a, sentence_b, tokenizer, max_len):\n",
        "\n",
        "  tokens_a = tokenizer.encode(sentence_a, add_special_tokens=False, max_length=96, truncation=True)\n",
        "  tokens_b = tokenizer.encode(sentence_b, add_special_tokens=False, max_length=96, truncation=True)\n",
        "\n",
        "  if (len(tokens_a) + len(tokens_b)) > (max_len - 3):\n",
        "    excess = len(tokens_a) + len(tokens_b) - (max_len - 3)\n",
        "    dif = len(tokens_a) - len(tokens_b)\n",
        "    if dif < 0:\n",
        "      dif = -dif\n",
        "    if dif > 25:\n",
        "      if len(tokens_a) > len(tokens_b):\n",
        "        tokens_a = tokens_a[:-excess]\n",
        "      else:\n",
        "        tokens_b = tokens_b[:-excess]\n",
        "    else:\n",
        "      excess_a = round(excess/2)\n",
        "      excess_b = excess - excess_a\n",
        "      if excess_a != 0:\n",
        "        tokens_a = tokens_a[:-excess_a]\n",
        "      tokens_b = tokens_b[:-excess_b]\n",
        "  \n",
        "  masked_a, label_a = random_word(tokens_a, tokenizer)\n",
        "  masked_b, label_b = random_word(tokens_b, tokenizer)\n",
        "\n",
        "  masked = [tokenizer.cls_token_id] + masked_a + [tokenizer.sep_token_id] + masked_b + [tokenizer.sep_token_id]\n",
        "  mask_labels = [-100] + label_a + [-100] + label_b + [-100]\n",
        "  attention_mask = torch.cat((torch.ones(len(masked), dtype=torch.int64), torch.zeros(max_len - len(masked), dtype=torch.int64)))\n",
        "  token_type_ids = torch.cat((torch.zeros(len(masked_a)+2, dtype=torch.int64), torch.ones(len(masked_b)+1, dtype=torch.int64), torch.zeros(max_len - len(masked), dtype=torch.int64)))\n",
        "  input_ids = torch.cat((torch.tensor(masked, dtype=torch.int64), torch.zeros(max_len - len(masked), dtype=torch.int64)))\n",
        "  mask_labels = torch.cat((torch.tensor(mask_labels), torch.empty(max_len - len(mask_labels), dtype=torch.int64).fill_(-100)))\n",
        "  \n",
        "  return input_ids, token_type_ids, attention_mask, mask_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkyVgE-I2Lqz"
      },
      "source": [
        "class DAPTDataset(data.Dataset):\n",
        "\n",
        "  def __init__(self, sentences_a, sentences_b, is_next_labels, tokenizer, max_len):\n",
        "    self.sentences_a = sentences_a\n",
        "    self.sentences_b = sentences_b\n",
        "    self.is_next_labels = is_next_labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sentences_a)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    sentence_a = self.sentences_a[item]\n",
        "    sentence_b = self.sentences_b[item]\n",
        "    is_next = self.is_next_labels[item]\n",
        "\n",
        "    input_ids, token_type_ids, attention_mask, mask_labels = tokenize_pairs(sentence_a, sentence_b, tokenizer=self.tokenizer, max_len=self.max_len)\n",
        "\n",
        "    return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_mask,\n",
        "      \"is_next\": is_next,\n",
        "      \"mask_labels\": mask_labels\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1JNmjJ63h0s"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = DAPTDataset(\n",
        "      sentences_a=df.sentence_a.to_numpy(),\n",
        "      sentences_b=df.sentence_b.to_numpy(),\n",
        "      is_next_labels=df.is_next.to_numpy(),\n",
        "      tokenizer=tokenizer,\n",
        "      max_len=max_len,\n",
        "  )\n",
        "\n",
        "  return data.DataLoader(\n",
        "      ds,\n",
        "      batch_size=batch_size,\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RBx725030N9"
      },
      "source": [
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBXNSirYIr6U"
      },
      "source": [
        "# Define model\n",
        "the model calculates the loss for MLM and NSP and concatenates it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYa-_FzsZfR6"
      },
      "source": [
        "class DAPTPreTrainer(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "      super(DAPTPreTrainer, self).__init__()\n",
        "      self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "      self.linear2 = nn.Linear(self.bert.config.hidden_size, 2)\n",
        "      self.cls = BertOnlyMLMHead(self.bert.config)\n",
        "\n",
        "  def forward(self, input_ids, token_type_ids, attention_mask, mask_labels, is_next):\n",
        "    outputs = self.bert(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids\n",
        "    )\n",
        "\n",
        "    sequence_output = outputs[0]\n",
        "    pooler_output = outputs[1]\n",
        "\n",
        "    nsp_scores = self.linear2(pooler_output)\n",
        "\n",
        "    hidden_size = sequence_output.size(-1)\n",
        "    mask_labels = mask_labels.reshape(-1)\n",
        "    labels_mask = mask_labels != -100\n",
        "    selected_mask_labels = mask_labels[labels_mask]\n",
        "    sequence_output = sequence_output.view(-1, hidden_size)\n",
        "    selected_sequence_output = sequence_output.masked_select(labels_mask.unsqueeze(1)).view(-1, hidden_size)\n",
        "    mlm_scores = self.cls(selected_sequence_output)\n",
        "\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "\n",
        "    mlm_loss = loss_fn(mlm_scores, selected_mask_labels)\n",
        "    mlm_preds = torch.argmax(mlm_scores, dim=1)\n",
        "    mlm_results = mlm_preds == selected_mask_labels\n",
        "\n",
        "    nsp_loss = loss_fn(nsp_scores, is_next)\n",
        "    nsp_preds = torch.argmax(nsp_scores, dim=1)\n",
        "    nsp_results = nsp_preds == is_next\n",
        "\n",
        "    return mlm_loss, mlm_results.tolist(), nsp_loss, nsp_results.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkNqasZ5p8tm"
      },
      "source": [
        "model = DAPTPreTrainer().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZxNXOmTJPP0"
      },
      "source": [
        "# Training and validation\n",
        "\n",
        "Here the actual DAPT happens. First, the hyperparameters and some metrics are defined. Then the training and evaluation functions are defined and called. During the training process, the loss and accuracy for MLM and NSP are returned. On a Tesla P100 this should take ~5 hours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h98CKPRH-pDp"
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps= 0.1 * total_steps,\n",
        "  num_training_steps= 0.9 * total_steps\n",
        ")\n",
        "loss_fn = CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM-bEG_t__Rq"
      },
      "source": [
        "def train_epoch(model, data_loader, optimizer, device, scheduler):\n",
        "\n",
        "  model = model.train()\n",
        "\n",
        "  mlm_losses = []\n",
        "  mlm_predictions = []\n",
        "  nsp_losses = []\n",
        "  nsp_predictions = []\n",
        "\n",
        "  for batch in data_loader:\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    mask_labels = batch[\"mask_labels\"].to(device)\n",
        "    token_type_ids = batch[\"token_type_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "    is_next = batch[\"is_next\"].to(device)\n",
        "\n",
        "    mlm_loss, mlm_results, nsp_loss, nsp_results = model(input_ids, token_type_ids, attention_mask, mask_labels, is_next)\n",
        "    mlm_losses.append(mlm_loss.item())\n",
        "    nsp_losses.append(nsp_loss.item())\n",
        "    mlm_predictions = mlm_predictions + mlm_results\n",
        "    nsp_predictions = nsp_predictions + nsp_results\n",
        "    loss = mlm_loss + nsp_loss\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return np.mean(mlm_losses), sum(mlm_predictions)/len(mlm_predictions), np.mean(nsp_losses), sum(nsp_predictions)/len(nsp_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vUCk6d8XyKV"
      },
      "source": [
        "def eval_model(model, data_loader, device):\n",
        "\n",
        "  model = model.eval()\n",
        "\n",
        "  mlm_losses = []\n",
        "  mlm_predictions = []\n",
        "  nsp_losses = []\n",
        "  nsp_predictions = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      input_ids = batch[\"input_ids\"].to(device)\n",
        "      mask_labels = batch[\"mask_labels\"].to(device)\n",
        "      token_type_ids = batch[\"token_type_ids\"].to(device)\n",
        "      attention_mask = batch[\"attention_mask\"].to(device)\n",
        "      is_next = batch[\"is_next\"].to(device)\n",
        "\n",
        "      mlm_loss, mlm_results, nsp_loss, nsp_results = model(input_ids, token_type_ids, attention_mask, mask_labels, is_next)\n",
        "      mlm_losses.append(mlm_loss.item())\n",
        "      nsp_losses.append(nsp_loss.item())\n",
        "      mlm_predictions = mlm_predictions + mlm_results\n",
        "      nsp_predictions = nsp_predictions + nsp_results\n",
        "\n",
        "  return np.mean(mlm_losses), sum(mlm_predictions)/len(mlm_predictions), np.mean(nsp_losses), sum(nsp_predictions)/len(nsp_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i97mg1nZDP7s"
      },
      "source": [
        "%%time\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_mlm_loss, train_mlm_acc, train_nsp_loss, train_nsp_acc = train_epoch(\n",
        "      model,\n",
        "      train_data_loader,\n",
        "      optimizer,\n",
        "      device,\n",
        "      scheduler\n",
        "  )\n",
        "\n",
        "  print(f\"Train mlm loss {train_mlm_loss} mlm accuracy {train_mlm_acc}\")\n",
        "  print(f\"Train nsp loss {train_nsp_loss} nsp accuracy {train_nsp_acc}\")\n",
        "  print()\n",
        "\n",
        "  val_mlm_loss, val_mlm_acc, val_nsp_loss, val_nsp_acc = eval_model(\n",
        "      model,\n",
        "      test_data_loader,\n",
        "      device\n",
        "  )\n",
        "\n",
        "  print(f\"Val   mlm loss {val_mlm_loss} mlm accuracy {val_mlm_acc}\")\n",
        "  print(f\"Val   nsp loss {val_nsp_loss} nsp accuracy {val_nsp_acc}\")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN-qm9JtEfYn"
      },
      "source": [
        "model.bert.save_pretrained(\"bert_mlm_nsp_cased\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}